{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q )) what is language processing and its applications?\n",
    "A )) Natural language processing (NLP) is the ability of a computer program to understand human language as it is spoken. \n",
    "     NLP is a component of artificial intelligence (AI).\n",
    "     \n",
    "    following are the basic  applications of nlp -\n",
    "     ~Speech recognition system\n",
    "     ~Question answering system\n",
    "     ~Translation from one specific language to another specific language\n",
    "     ~Text summarization\n",
    "     ~Sentiment analysis\n",
    "     ~Template-based chatbots\n",
    "     ~Text classification\n",
    "         \n",
    "#NLTK is the most useable library for nlp in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q )) what is corpus and corpora ?\n",
    "A )) Corpus is a collection of written or spoken natural language material, stored on computer, and used to find out how \n",
    "     language is used.You can say that a large collection of data is called corpus . In any NLP application, we need data \n",
    "     or corpus to building NLP tools and applications. \n",
    "         corpus is a collection of written texts and corpora is the plural of corpus.\n",
    "    \n",
    "    nltk has four types of corpora. Let's look at each of them:\n",
    "    1) Isolate corpus: This type of corpus is a collection of text or natural language. Examples of this kind of corpus are \n",
    "        gutenberg(print), webtext, and so on\n",
    "    2)Categorized corpus: This type of corpus is a collection of texts that are grouped into different types of categories.\n",
    "    3)Overlapping corpus: This type of corpus is a collection of texts that are categorized, but the categories overlap \n",
    "        with each other.\n",
    "    4)Temporal corpus: This type of corpus is a collection of the usages of natural language over a period of time. \n",
    "         An example of this kind of corpus is the inaugural address corpus. Suppose you recode the usage of a language in any\n",
    "        city of India in 1950. Then you repeat the same activity to see the usage of the language in that particular city in\n",
    "        1980 and  then again in 2017. You will have recorded the various data attributes regarding how people used\n",
    "        the language and what the  changes over a period of time were.\n",
    "\n",
    "#The nltk library provides some inbuilt corpus\n",
    "#to list down all the courpus we have in nltk just use this command\n",
    "#import nltk.corpus\n",
    "#dir(nltk.corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q )) what is Natural language understanding (NLU) and natural language generation (NLG) ?\n",
    "A ))  Natural language understanding (NLU)  -\n",
    "       1) This component helps to explain the meaning behind the NL, whether it is written text or in speech format.\n",
    "           We can analyze English, French, Spanish, Hindi, or any other human language.\n",
    "       2) NLU generates facts from NL by using various tools and techniques, such as POS tagger, parsers, and so on,\n",
    "          in order to develop NLP applications.\n",
    "       3)It is the process of reading and interpreting language. \n",
    "        \n",
    "       Natural language generation (NLG) -\n",
    "       1)This component helps to generate the NL using machines.\n",
    "       2)NLG start from facts like POS tags, parsing results, and so on to generate the NL.\n",
    "       3)It is the process of writing or generating language.\n",
    "        \n",
    "        \n",
    "       NLU requires the following analysis to convert NL into a useful representation:\n",
    "        ~Morphological analysis\n",
    "        ~Lexical analysis\n",
    "        ~Syntactic analysis\n",
    "        ~Semantic analysis\n",
    "        ~Handling ambiguity\n",
    "        ~Discourse integration\n",
    "        ~Pragmatic analysis\n",
    "# the analysis of language is called Linguistics\n",
    "\n",
    "1) Morphological analysis -\n",
    "   Morphology is branch of linguistics that studies how words can be structured and formed . In linguistics, a morpheme is \n",
    "   the smallest meaningful unit of a given language. The important part of morphology is morphemes, which are the basic unit \n",
    "   of morphology.Let's take an example. The word boy consists of single morpheme whereas boys consists of two morphemes; \n",
    "   one is boy and the other morpheme -s\n",
    "   Morphological analysis is defined as grammatical analysis of how words are formed by using morphemes, which are the minimum \n",
    "   unit of meaning.\n",
    "   Those affixes can be divided into four types:\n",
    "    ~Prefixes, which appear before a stem, such as un-happy\n",
    "    ~Suffixes, which appear after a stem, such as happi-ness\n",
    "    ~Infixes, which appear inside a stem, such as b-um-ili (this means buy in Tagalog, a language from the Philippines)\n",
    "    ~Circumfixes surround a word. It is attached to the beginning and end of the stem.For example, ka-baddang-an \n",
    "     (this means help in Tuwali Ifugao, another languagefrom the Philippines)\n",
    "# This smallest single isolated part of a sentence is called a word its is diffferent from morphological analysis\n",
    "\n",
    "2) Lexical analysis -\n",
    "   Lexical analysis is defined as the process of breaking down a text into words, phrases, and other meaningful elements. \n",
    "   Lexical analysis is based on word-level analysis. In this kind of analysis, we also focus on the meaning of the words, \n",
    "   phrases, and other elements, such as symbols.Sometimes, lexical analysis is also loosely described as the tokenization \n",
    "   process.\n",
    "          Tokens are defined as the meaningful elements that are generated by using techniques of lexical analysis.\n",
    "    \n",
    "3) Syntactic analysis -\n",
    "   Syntactic analysis is defined as analysis that tells us the logical meaning of certain given sentences or parts of those \n",
    "    sentences. We also need to consider rules of grammar in order to define the logical meaning as well as correctness of \n",
    "    the sentences.\n",
    "            Let's take an example: If I'm considering English and I have a sentence such as School go a boy, this sentence \n",
    "    does not logically convey its meaning, and its grammatical structure is not correct. So, syntactic analysis tells us \n",
    "    whether the given sentence conveys its logical meaning and whether its grammatical structure is correct.\n",
    "            Syntactic analysis is a well-developed area of NLP that deals with the syntax of NL. In syntactic analysis, \n",
    "    grammar  rules have been used to determine which sentences are egitimate. The grammar has been applied in order to develop \n",
    "    a parsing algorithm to produce a structure representation or a parse tree.\n",
    "\n",
    "4) Semantic analysis -\n",
    "    If you have a sentence such as the white house is great, this can mean the statement is in context of the White House in \n",
    "    the USA, whereas it is also possible the statement is literally talking about a house nearby, whose color is white is \n",
    "    great. So, getting the proper meaning of the sentence is the task of semantic analysis.\n",
    "        Semantic analysis is generating representation for meaning of the NL. You might think, if lexical analysis also focuses \n",
    "    on the meaning of the words given in stream of text, then what is the difference between semantic analysis and lexical \n",
    "    analysis? The answer is that lexical analysis is based on smaller tokens; its focus is on meaning of the words, but \n",
    "    semantic analysis focuses on larger chunks. Semantic analysis can be performed at the phrase level, sentence level, \n",
    "    paragraph level, and sometimes at the document level as well. \n",
    "  \n",
    "5) Handling ambiguity -\n",
    "#Ambiguity is when the meaning of a word, phrase, or sentence is uncertain. ... However, sometimes ambiguity is used \n",
    "deliberately to #add humor to a text. Examples of Ambiguity: Sarah gave a bath to her dog wearing a pink t-shirt.\n",
    "\n",
    "   When we jump into semantic analysis, we may find there are many cases that are too ambiguous for an NLP system to handle. \n",
    "    In    these cases, we need to know what kinds of ambiguity exist and how we can handle them.If an expression\n",
    "    (word/phrase/sentence)    has more than one interpretation we can refer it as ambiguous.A word, phrase, or sentence is \n",
    "    ambiguous if it has more than      one meaning. If we consider word light,than it can mean not very heavy or not very dark.\n",
    "        It is of 4 types-\n",
    "        a)Lexical ambiguity\n",
    "           Lexical ambiguity is word-level ambiguity. A single word can have ambiguous meaning in terms of its internal\n",
    "            structure and its syntactic class. Let's look at some examples:\n",
    "            Sentence 1: Look at the stars. Here, look is a verb.\n",
    "            Sentence 2: The person gave him a warm look. Here, look is a noun.\n",
    "        b)Syntactic ambiguity  -\n",
    "           We have seen, in syntactic analysis, sequences of words are grammatically structured. There are different ways of\n",
    "            interpreting sequences of words, and each structure has a different interpretation. In syntactic ambiguity, syntax\n",
    "            is unclear, not the word-level meaning. Here is an example of structural ambiguity:\n",
    "           example -\n",
    "             The man saw the girl with the telescope. Here, the ambiguity is because it is not clear whether the man sees the  \n",
    "            girl, who has a telescope, or the man sees the girl by using telescope.\n",
    "         c) Semantic ambiguity -\n",
    "             Semantic ambiguity occurs when the meaning of the words themselves can be misinterpreted. Here's an example:\n",
    "               Sentence 1:ABC head seeks arms\n",
    "               Sentence 2:Here, the word head either means chief or body part, and in the same way, arms can be interpreted as \n",
    "                weapons or as body parts\n",
    "               Sentence 3:This kind of ambiguity is considered in semantic ambiguity.\n",
    "         d) Pragmatic ambiguity -\n",
    "              Pragmatics ambiguity occurs when the context of a phrase gives it multiple different interpretations.\n",
    "               Let's take an example:\n",
    "                 Give it to that girl. This could mean any number of things.\n",
    "\n",
    "6) Discourse integration -\n",
    "   Discourse integration is closely related to pragmatics. Discourse integration is considered as the larger context for any \n",
    "    smaller part of NL structure. NL is so complex and, most of the time, sequences of text are dependent on prior discourse.\n",
    "    This concept occurs often in pragmatic ambiguity. This analysis deals with how the immediately preceding sentence    can \n",
    "    affect the meaning and interpretation of the next sentence. Here, context can be analyzed in a bigger context, such as \n",
    "    paragraph level, document level, and so on\n",
    "   \n",
    "7)Pragmatic analysis -\n",
    "  Pragmatic analysis deals with outside word knowledge, which means knowledge that is external to the documents and/or queries.\n",
    "    Pragmatics analysis that focuses on what was described is reinterpreted by what it actually meant, deriving the various  \n",
    "    aspects of language that require real world knowledge.\n",
    "      Let's look at an example:\n",
    "         Pruning a tree is a long process\n",
    "         \n",
    "      Here, pruning a tree is one of the concepts of computer science algorithm techniques. So, the word pruning is not \n",
    "    related       to cutting the actual physical tree,we are talking about computer science algorithm. This is an ambiguous\n",
    "    situation; how       to deal with these kinds of ambiguous situations is also an open area of research. Big tech giants\n",
    "    use deep learning           techniques to do pragmatics analysis and try to generate the accurate context of the sentence \n",
    "    in order to develop highly accurate NLP applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q )) What is the difference between a stem and a root?\n",
    "A )) Stem -\n",
    "     ~In order to generate a stem, we need to remove affixes from the word\n",
    "     ~From the stem, we can generate the root by further dividing it\n",
    "     ~The word Untie is stem\n",
    "     \n",
    "     Root -\n",
    "    ~A root cannot be further divided into smaller morphemes\n",
    "    ~A stem is generated by using a root plus derivational morphemes\n",
    "    ~The word tie is root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q )) Difference between stemming and lemmatization?\n",
    "A )) Stemming -\n",
    "     ~Stemming usually operates on single word without knowledge of the context\n",
    "     ~In stemming, we do not consider POS tags\n",
    "     ~Stemming is used to group words with a similar basic meaning together\n",
    "#in short  Stemming is a rudimentary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word.   \n",
    "\n",
    "     Lemmatization -\n",
    "    ~Lemmatization usually considers words and the context of the word in the sentence\n",
    "    ~In lemmatization, we consider POS tags . \n",
    "    ~Lemmatization concept is used to make dictionary or WordNet kind of dictionary.\n",
    "#in short Lemmatization, on the other hand, is an organized & step by step procedure of obtaining the root form of the \n",
    "#word, it makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and \n",
    "#grammar  relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q )) what is context free grammer and parse tree ?\n",
    "A ))  A context-free grammar (CFG) is a certain type of formal grammar: a set of production rules that describe all \n",
    "      possible strings in a given formal language.\n",
    "      \n",
    "      Context free grammar G can be defined by four tuples as: G= (N, T, R, S)\n",
    "            ~G describes the grammar\n",
    "            ~T describes a finite set of terminal symbols.\n",
    "            ~N describes a finite set of non-terminal symbols\n",
    "            ~R describes a set of production rules\n",
    "            ~S is the start symbol.\n",
    "      \n",
    "      Parse  tree is a graphical representation of of sentence using grammar rules. It generates tree like structure of the\n",
    "      sentence -\n",
    "      lets discuss symbol first of parse tree -\n",
    "          ~S stands for sentence\n",
    "          ~NP stands for noun phrase\n",
    "          ~VP stands for verb phrase\n",
    "          ~V stands for verb\n",
    "          ~N stand for noun\n",
    "          ~ART stands for article a, an, or the\n",
    "# for parse tree diagram search google it is easily available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q ))What are part of speech tags?\n",
    "A ))A part of speech is a category of words or lexical items that have similar grammatical properties. Words belonging to \n",
    "    the same part of speech (POS) category have similar behavior within the grammatical structure of sentences. In English, \n",
    "    POS categories are verb, noun, adjective, adverb, pronoun, preposition, conjunction, interjection, and sometimes numeral, \n",
    "    article, or determiner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q )) what is Feature engineering ?\n",
    "A )) Feature engineering is the process of generating or deriving features (attributes or an individual measurable property\n",
    "     of a phenomenon) from raw data or corpus that will help us develop NLP applications or solve NLP-related problems.\n",
    "          If you're wondering what information can be a feature, then the answer is that any attribute can be a feature \n",
    "     as long as it is useful in order to generate a good ML model that will produce the output for NLP applications accurately \n",
    "     and efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q )) what is Parsers ? \n",
    "A ))  A parser actually analyzes the sentences using the rules of context-free grammar or probabilistic context-free grammar.\n",
    "\n",
    "Q )) what is probabilistic context-free grammar ?\n",
    "A )) Context free grammar G can be defined by four tuples as: G= (N, T, R, S ,P)\n",
    "            ~G describes the grammar\n",
    "            ~T describes a finite set of terminal symbols.\n",
    "            ~N describes a finite set of non-terminal symbols\n",
    "            ~R describes a set of production rules\n",
    "            ~S is the start symbol.\n",
    "            ~P is the probability function\n",
    "     the only new thing here is the probability function,so let's look at that here, the probability function takes each \n",
    "     grammar rule and gives us the probability value of each rule. This probability maps to a real number, R. The range \n",
    "     for R is between 0 and 1. We are not blindly taking any probability value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q )) what is Normalization?\n",
    "A )) Normalization generally refers to a series of related tasks meant to put all text on a level playing field: converting \n",
    "     all  text to the same case (upper or lower), removing punctuation, converting numbers to their word equivalents, and \n",
    "     so on. Normalization puts all words on equal footing, and allows processing to proceed uniformly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lean BOW(bag of words)\n",
    "#to this\n",
    "#https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q )) What do you mean by regular expression ?\n",
    "A )) Simply put, regular expression is a sequence of character(s) mainly used to find and replace patterns in a string \n",
    "      or file.\n",
    "    \n",
    "    followig various methods of Regular Expressions-\n",
    "         The ‘re’ package provides multiple methods to perform queries on an input string. Here are the most commonly used \n",
    "    methods, I will discuss:\n",
    "\n",
    "    1)re.match()\n",
    "    2)re.search()\n",
    "    3)re.findall()\n",
    "    4)re.split() \n",
    "    5)re.sub()\n",
    "    6)re.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEARN REGULAR EXPRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1)re.match()\n",
    "This method finds match if it occurs at start of the string. For example, calling match() on the string ‘AV Analytics AV’ and looking for a pattern ‘AV’ will match. However, if we look for only Analytics, the pattern will not match. Let’s perform it in python now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 2), match='AV'>\n"
     ]
    }
   ],
   "source": [
    "#Python has a built-in package called re, which can be used to work with Regular Expressions.\n",
    "import re\n",
    "#this output shows that  pattern has been found \n",
    "result = re.match('AV', 'AV Analytics Vidhya AV')\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matching string : AV\n"
     ]
    }
   ],
   "source": [
    "# printing the matching string\n",
    "#Placing r or R before a string literal creates what is known as a raw-string literal. Raw strings do not process escape \n",
    "      #sequences (\\n, \\b, etc.) and are thus commonly used for Regex patterns, which often contain a lot of \\ characters.\n",
    "result = re.match(r'AV', 'AV Analytics Vidhya AV')\n",
    "print('\\nMatching string :',result.group(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting position of the match : 0\n",
      "Ending position of the match : 2\n"
     ]
    }
   ],
   "source": [
    "# There are methods like start() and end() to know the start and end position of matching pattern in the string.\n",
    "result = re.match(r'AV', 'AV Analytics Vidhya AV')\n",
    "print ('Starting position of the match :',result.start())\n",
    "print ('Ending position of the match :',result.end())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2)re.search()\n",
    "re.search(pattern, string):\n",
    "It is similar to match() but it doesn’t restrict us to find matches at the beginning of the string only. Unlike previous \n",
    "method, here searching for pattern ‘Analytics’ will return a match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytics\n"
     ]
    }
   ],
   "source": [
    "result = re.search(r'Analytics', 'AV Analytics Vidhya AV')\n",
    "print(result.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3)re.findall()\n",
    "It helps to get a list of all matching patterns. It has no constraints of searching from start or end. If we will use method\n",
    "findall to search ‘AV’ in given string it will return both occurrence of AV. While searching a string, I would recommend you\n",
    "to use re.findall() always, it can work like re.search() and re.match() both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AV', 'AV']\n"
     ]
    }
   ],
   "source": [
    "result = re.findall(r'AV', 'AV Analytics Vidhya AV')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4)re.split()\n",
    "re.split(pattern, string, [maxsplit=0]):\n",
    "This methods helps to split string by the occurrences of given pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anal', 'tics']"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=re.split(r'y','Analytics')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Analyt', 'cs Vidhya']"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Method split() has another argument “maxsplit“. It has default value of zero. In this case it does the maximum splits that\n",
    "#     can be done, but if we give value to maxsplit\n",
    "result=re.split(r'i','Analytics Vidhya',maxsplit=1)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5)re.sub()\n",
    "It helps to search a pattern and replace with a new sub string. If the pattern is not found, string is returned unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AV is largest Analytics community of the World'"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=re.sub(r'India','the World','AV is largest Analytics community of India')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6)re.compile()\n",
    "We can combine a regular expression pattern into pattern objects, which can be used for pattern matching. It also helps to search a pattern again without rewriting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AV', 'AV']\n",
      "['AV']\n"
     ]
    }
   ],
   "source": [
    "pattern=re.compile('AV')\n",
    "result=pattern.findall('AV Analytics Vidhya AV')\n",
    "print(result)\n",
    "result2=pattern.findall('AV is largest analytics community of India')\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What are the most commonly used operators in regular expression?\n",
    "regular expressions can specify patterns, not just fixed characters. Here are the most commonly used operators that helps to \n",
    "generate an expression to represent required characters in a string or file. It is commonly used in web scrapping and  text \n",
    "mining to extract required information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'V', 'i', 's', 'l', 'a', 'r', 'g', 'e', 's', 't', 'A', 'n', 'a', 'l', 'y', 't', 'i', 'c', 's', 'c', 'o', 'm', 'm', 'u', 'n', 'i', 't', 'y', 'o', 'f', 'I', 'n', 'd', 'i', 'a']\n"
     ]
    }
   ],
   "source": [
    "#Some Examples of Regular Expressions\n",
    "\n",
    "#Problem 1: Return the first word of a given string\n",
    "\n",
    "#a) Extract each character (using “\\w“)\n",
    "\n",
    "result=re.findall(r'\\w','AV is largest Analytics community of India')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AV', '', 'is', '', 'largest', '', 'Analytics', '', 'community', '', 'of', '', 'India', '']\n"
     ]
    }
   ],
   "source": [
    "# Extract each word (using “*” or “+“)\n",
    "result=re.findall(r'\\w*','AV is largest Analytics community of India')\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AV', 'is', 'la', 'rg', 'es', 'An', 'al', 'yt', 'ic', 'co', 'mm', 'un', 'it', 'of', 'In', 'di']\n"
     ]
    }
   ],
   "source": [
    "#2Problem 2: Return the first two character of each word\n",
    "#  Extract consecutive two characters of each word, excluding spaces (using “\\w“)\n",
    "result=re.findall(r'\\w\\w','AV is largest Analytics community of India')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@gmail', '@test', '@analyticsvidhya', '@rest']\n"
     ]
    }
   ],
   "source": [
    "#Problem 3: Return the domain type of given email-ids\n",
    "# Extract all characters after “@”\n",
    "result=re.findall(r'@\\w+','abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz') \n",
    "print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['com', 'in', 'com', 'biz']\n"
     ]
    }
   ],
   "source": [
    "#Extract only domain name using “( )”\n",
    "result=re.findall(r'@\\w+.(\\w+)','abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz')\n",
    "print( result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12-05-2007', '11-11-2011', '12-01-2009']\n"
     ]
    }
   ],
   "source": [
    "#Problem 4: Return date from given string\n",
    "#Here we will use “\\d” to extract digit.\n",
    "result=re.findall(r'\\d{2}-\\d{2}-\\d{4}','Amit 34-3456 12-05-2007, XYZ 56-4532 11-11-2011, ABC 67-8945 12-01-2009')\n",
    "print( result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2007', '2011', '2009']\n"
     ]
    }
   ],
   "source": [
    "#If you want to extract only year again parenthesis “( )” will help you.\n",
    "result=re.findall(r'\\d{2}-\\d{2}-(\\d{4})','Amit 34-3456 12-05-2007, XYZ 56-4532 11-11-2011, ABC 67-8945 12-01-2009')\n",
    "print( result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AV', 'is', 'argest', 'Analytics', 'ommunity', 'of', 'India']\n"
     ]
    }
   ],
   "source": [
    "# Return all words of a string those starts with vowel\n",
    "result=re.findall(r'[aeiouAEIOU]\\w+','AV is largest Analytics community of India')\n",
    "print( result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "no\n",
      "no\n"
     ]
    }
   ],
   "source": [
    "# Problem 6: Validate a phone number (phone number must be of 10 digits and starts with 8 or 9) \n",
    "import re\n",
    "li=['9999999999','999999-999','99999x9999']\n",
    "for val in li:\n",
    " if re.match(r'[8-9]{1}[0-9]{9}',val) and len(val) == 10:\n",
    "     print ('yes')\n",
    " else:\n",
    "     print( 'no')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metacharacters\n",
    "#Metacharacters are characters with a special meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h', 'e', 'a', 'i', 'i', 'a', 'i']\n"
     ]
    }
   ],
   "source": [
    "#[]\tA set of characters \t\"[a-m]\"\n",
    "import re\n",
    "\n",
    "str = \"The rain in Spain\"\n",
    "\n",
    "#Find all lower case characters alphabetically between \"a\" and \"m\":\n",
    "\n",
    "x = re.findall(\"[a-m]\", str)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5', '9']\n"
     ]
    }
   ],
   "source": [
    "#\\\tSignals a special sequence (can also be used to escape special characters)\t\"\\d\"\n",
    "import re\n",
    "\n",
    "str = \"That will be 59 dollars\"\n",
    "\n",
    "#Find all digit characters:\n",
    "\n",
    "x = re.findall(\"\\d\", str)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello']\n"
     ]
    }
   ],
   "source": [
    "#.\tAny character (except newline character)\t\"he..o\"\n",
    "import re\n",
    "\n",
    "str = \"hello world\"\n",
    "\n",
    "#Search for a sequence that starts with \"he\", followed by two (any) characters, and an \"o\":\n",
    "\n",
    "x = re.findall(\"he..o\", str)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the string starts with 'hello'\n"
     ]
    }
   ],
   "source": [
    "#^\tStarts with\t\"^hello\"\n",
    "import re\n",
    "\n",
    "str = \"hello world\"\n",
    "\n",
    "#Check if the string starts with 'hello':\n",
    "\n",
    "x = re.findall(\"^hello\", str)\n",
    "if (x):\n",
    "  print(\"Yes, the string starts with 'hello'\")\n",
    "else:\n",
    "  print(\"No match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the string ends with 'world'\n"
     ]
    }
   ],
   "source": [
    "#$\tEnds with\t\"world$\"\n",
    "import re\n",
    "\n",
    "str = \"hello world\"\n",
    "\n",
    "#Check if the string ends with 'world':\n",
    "\n",
    "x = re.findall(\"world$\", str)\n",
    "if (x):\n",
    "  print(\"Yes, the string ends with 'world'\")\n",
    "else:\n",
    "  print(\"No match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ai', 'ai', 'ai', 'ai']\n",
      "Yes, there is at least one match!\n"
     ]
    }
   ],
   "source": [
    "# *\tZero or more occurrences\t\"aix*\"\n",
    "import re\n",
    "\n",
    "str = \"The rain in Spain falls mainly in the plain!\"\n",
    "\n",
    "#Check if the string contains \"ai\" followed by 0 or more \"x\" characters:\n",
    "\n",
    "x = re.findall(\"aix*\", str)\n",
    "\n",
    "print(x)\n",
    "\n",
    "if (x):\n",
    "  print(\"Yes, there is at least one match!\")\n",
    "else:\n",
    "  print(\"No match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "No match\n"
     ]
    }
   ],
   "source": [
    "# +\tOne or more occurrences\t\"aix+\"\n",
    "\n",
    "import re\n",
    "\n",
    "str = \"The rain in Spain falls mainly in the plain!\"\n",
    "\n",
    "#Check if the string contains \"ai\" followed by 1 or more \"x\" characters:\n",
    "\n",
    "x = re.findall(\"aix+\", str)\n",
    "\n",
    "print(x)\n",
    "\n",
    "if (x):\n",
    "  print(\"Yes, there is at least one match!\")\n",
    "else:\n",
    "  print(\"No match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all']\n",
      "Yes, there is at least one match!\n"
     ]
    }
   ],
   "source": [
    "# {}\tExactly the specified number of occurrences\t\"al{2}\"\n",
    "\n",
    "import re\n",
    "\n",
    "str = \"The rain in Spain falls mainly in the plain!\"\n",
    "\n",
    "#Check if the string contains \"a\" followed by exactly two \"l\" characters:\n",
    "/\n",
    "x = re.findall(\"al{2}\", str)\n",
    "\n",
    "print(x)\n",
    "\n",
    "if (x):\n",
    "  print(\"Yes, there is at least one match!\")\n",
    "else:\n",
    "  print(\"No match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['falls']\n",
      "Yes, there is at least one match!\n"
     ]
    }
   ],
   "source": [
    "#|\tEither or\t\"falls|stays\" \n",
    "\n",
    "import re\n",
    "\n",
    "str = \"The rain in Spain falls mainly in the plain!\"\n",
    "\n",
    "#Check if the string contains either \"falls\" or \"stays\":\n",
    "\n",
    "x = re.findall(\"falls|stays\", str)\n",
    "\n",
    "print(x)\n",
    "\n",
    "if (x):\n",
    "  print(\"Yes, there is at least one match!\")\n",
    "else:\n",
    "  print(\"No match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special Sequences\n",
    "# A special sequence is a \\ followed by one of the characters in the list below, and has a special meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The']\n",
      "Yes, there is a match!\n"
     ]
    }
   ],
   "source": [
    "# \\A\tReturns a match if the specified characters are at the beginning of the string\t\"\\AThe\"\n",
    "\n",
    "import re\n",
    "\n",
    "str = \"The rain in Spain\"\n",
    "\n",
    "#Check if the string starts with \"The\":\n",
    "\n",
    "x = re.findall(\"\\AThe\", str)\n",
    "\n",
    "print(x)\n",
    "\n",
    "if (x):\n",
    "  print(\"Yes, there is a match!\")\n",
    "else:\n",
    "  print(\"No match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "No match\n",
      "['ain', 'ain']\n",
      "Yes, there is at least one match!\n"
     ]
    }
   ],
   "source": [
    "# \\b\tReturns a match where the specified characters are at the beginning or at the end of a word\tr\"\\bain\" r\"ain\\b\"\n",
    "import re\n",
    "\n",
    "str = \"The rain in Spain\"\n",
    "\n",
    "#Check if \"ain\" is present at the beginning of a WORD:\n",
    "\n",
    "x = re.findall(r\"\\bain\", str)\n",
    "\n",
    "print(x)\n",
    "\n",
    "if (x):\n",
    "  print(\"Yes, there is at least one match!\")\n",
    "else:\n",
    "  print(\"No match\")\n",
    "import re\n",
    "\n",
    "str = \"The rain in Spain\"\n",
    "\n",
    "#Check if \"ain\" is present, but NOT at the beginning of a word:\n",
    "\n",
    "x = re.findall(r\"\\Bain\", str)\n",
    "\n",
    "print(x)\n",
    "\n",
    "if (x):\n",
    "  print(\"Yes, there is at least one match!\")\n",
    "else:\n",
    "  print(\"No match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ain', 'ain']\n",
      "Yes, there is at least one match!\n"
     ]
    }
   ],
   "source": [
    "##\\B\tReturns a match where the specified characters are present, but NOT at the beginning (or at the end) of a word\tr\"\\Bain\" \n",
    "# r\"ain\\B\"\n",
    "import re\n",
    "\n",
    "str = \"The rain in Spain\"\n",
    "\n",
    "#Check if \"ain\" is present, but NOT at the beginning of a word:\n",
    "\n",
    "x = re.findall(r\"\\Bain\", str)\n",
    "\n",
    "print(x)\n",
    "\n",
    "if (x):\n",
    "  print(\"Yes, there is at least one match!\")\n",
    "else:\n",
    "  print(\"No match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ain', 'ain']\n",
      "Yes, there is at least one match!\n"
     ]
    }
   ],
   "source": [
    "# \\d\tReturns a match where the string contains digits (numbers from 0-9)\t\"\\d\"\n",
    "\n",
    "import re\n",
    "\n",
    "str = \"The rain in Spain\"\n",
    "\n",
    "#Check if \"ain\" is present, but NOT at the beginning of a word:\n",
    "\n",
    "x = re.findall(r\"\\Bain\", str)\n",
    "\n",
    "print(x)\n",
    "\n",
    "if (x):\n",
    "  print(\"Yes, there is at least one match!\")\n",
    "else:\n",
    "  print(\"No match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'h', 'e', ' ', 'r', 'a', 'i', 'n', ' ', 'i', 'n', ' ', 'S', 'p', 'a', 'i', 'n']\n",
      "Yes, there is at least one match!\n"
     ]
    }
   ],
   "source": [
    "# \\D\tReturns a match where the string DOES NOT contain digits\t\"\\D\"\n",
    "\n",
    "import re\n",
    "\n",
    "str = \"The rain in Spain\"\n",
    "\n",
    "#Return a match at every no-digit character:\n",
    "\n",
    "x = re.findall(\"\\D\", str)\n",
    "\n",
    "print(x)\n",
    "\n",
    "if (x):\n",
    "  print(\"Yes, there is at least one match!\")\n",
    "else:\n",
    "  print(\"No match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ' ', ' ']\n",
      "Yes, there is at least one match!\n"
     ]
    }
   ],
   "source": [
    "# \\s\tReturns a match where the string contains a white space character\t\"\\s\"\n",
    "import re\n",
    "\n",
    "str = \"The rain in Spain\"\n",
    "\n",
    "#Return a match at every white-space character:\n",
    "\n",
    "x = re.findall(\"\\s\", str)\n",
    "\n",
    "print(x)\n",
    "\n",
    "if (x):\n",
    "  print(\"Yes, there is at least one match!\")\n",
    "else:\n",
    "  print(\"No match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'h', 'e', 'r', 'a', 'i', 'n', 'i', 'n', 'S', 'p', 'a', 'i', 'n']\n",
      "Yes, there is at least one match!\n"
     ]
    }
   ],
   "source": [
    "#\\S\tReturns a match where the string DOES NOT contain a white space character  \"\\S\"\n",
    "\n",
    "import re\n",
    "\n",
    "str = \"The rain in Spain\"\n",
    "\n",
    "#Return a match at every NON white-space character:\n",
    "\n",
    "x = re.findall(\"\\S\", str)\n",
    "\n",
    "print(x)\n",
    "\n",
    "if (x):\n",
    "  print(\"Yes, there is at least one match!\")\n",
    "else:\n",
    "  print(\"No match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'h', 'e', 'r', 'a', 'i', 'n', 'i', 'n', 'S', 'p', 'a', 'i', 'n']\n",
      "Yes, there is at least one match!\n"
     ]
    }
   ],
   "source": [
    "#\\w\tReturns a match where the string contains any word characters(characters from a to Z, digits from 0-9,\n",
    "# and the underscore _ character      \"\\w\"\n",
    "\n",
    "import re\n",
    "\n",
    "str = \"The rain in Spain\"\n",
    "\n",
    "#Return a match at every word character (characters from a to Z, digits from 0-9, and the underscore _ character):\n",
    "\n",
    "x = re.findall(\"\\w\", str)\n",
    "\n",
    "print(x)\n",
    "\n",
    "if (x):\n",
    "  print(\"Yes, there is at least one match!\")\n",
    "else:\n",
    "  print(\"No match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ' ', ' ']\n",
      "Yes, there is at least one match!\n"
     ]
    }
   ],
   "source": [
    "# \\W\tReturns a match where the string DOES NOT contain any word characters\t\"\\W\"\n",
    "\n",
    "import re\n",
    "\n",
    "str = \"The rain in Spain\"\n",
    "\n",
    "#Return a match at every NON word character (characters NOT between a and Z. Like \"!\", \"?\" white-space etc.):\n",
    "\n",
    "x = re.findall(\"\\W\", str)\n",
    "\n",
    "print(x)\n",
    "\n",
    "if (x):\n",
    "  print(\"Yes, there is at least one match!\")\n",
    "else:\n",
    "  print(\"No match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Spain']\n",
      "Yes, there is a match!\n"
     ]
    }
   ],
   "source": [
    "# \\Z\tReturns a match if the specified characters are at the end of the string\t\"Spain\\Z\"\n",
    "\n",
    "import re\n",
    "\n",
    "str = \"The rain in Spain\"\n",
    "\n",
    "#Check if the string ends with \"Spain\":\n",
    "\n",
    "x = re.findall(\"Spain\\Z\", str)\n",
    "\n",
    "print(x)\n",
    "\n",
    "if (x):\n",
    "  print(\"Yes, there is a match!\")\n",
    "else:\n",
    "  print(\"No match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sets\n",
    "A set is a set of characters inside a pair of square brackets [] with a special meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['r', 'a', 'n', 'n', 'a', 'n']\n",
      "Yes, there is at least one match!\n"
     ]
    }
   ],
   "source": [
    "#[arn]\tReturns a match where one of the specified characters (a, r, or n) are present\n",
    "import re\n",
    "\n",
    "str = \"The rain in Spain\"\n",
    "\n",
    "#Check if the string has any a, r, or n characters:\n",
    "\n",
    "x = re.findall(\"[arn]\", str)\n",
    "\n",
    "print(x)\n",
    "\n",
    "if (x):\n",
    "  print(\"Yes, there is at least one match!\")\n",
    "else:\n",
    "  print(\"No match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h', 'e', 'a', 'i', 'n', 'i', 'n', 'a', 'i', 'n']\n",
      "Yes, there is at least one match!\n"
     ]
    }
   ],
   "source": [
    "#[a-n]\tReturns a match for any lower case character, alphabetically between a and n\n",
    "import re\n",
    "\n",
    "str = \"The rain in Spain\"\n",
    "\n",
    "#Check if the string has any characters between a and n:\n",
    "\n",
    "x = re.findall(\"[a-n]\", str)\n",
    "\n",
    "print(x)\n",
    "\n",
    "if (x):\n",
    "  print(\"Yes, there is at least one match!\")\n",
    "else:\n",
    "  print(\"No match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'h', 'e', ' ', 'i', ' ', 'i', ' ', 'S', 'p', 'i']\n",
      "Yes, there is at least one match!\n"
     ]
    }
   ],
   "source": [
    "# [^arn]\tReturns a match for any character EXCEPT a, r, and n\n",
    "\n",
    "import re\n",
    "\n",
    "str = \"The rain in Spain\"\n",
    "\n",
    "#Check if the string has other characters than a, r, or n:\n",
    "\n",
    "x = re.findall(\"[^arn]\", str)\n",
    "\n",
    "print(x)\n",
    "\n",
    "if (x):\n",
    "  print(\"Yes, there is at least one match!\")\n",
    "else:\n",
    "  print(\"No match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "No match\n"
     ]
    }
   ],
   "source": [
    "#[0123]\tReturns a match where any of the specified digits (0, 1, 2, or 3) are present \n",
    "import re\n",
    "\n",
    "str = \"The rain in Spain\"\n",
    "\n",
    "#Check if the string has any 0, 1, 2, or 3 digits:\n",
    "\n",
    "x = re.findall(\"[0123]\", str)\n",
    "\n",
    "print(x)\n",
    "\n",
    "if (x):\n",
    "  print(\"Yes, there is at least one match!\")\n",
    "else:\n",
    "  print(\"No match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8', '1', '1', '4', '5']\n",
      "Yes, there is at least one match!\n"
     ]
    }
   ],
   "source": [
    "#[0-9]\tReturns a match for any digit between 0 and 9\n",
    "import re\n",
    "\n",
    "str = \"8 times before 11:45 AM\"\n",
    "\n",
    "#Check if the string has any digits:\n",
    "\n",
    "x = re.findall(\"[0-9]\", str)\n",
    "\n",
    "print(x)\n",
    "\n",
    "if (x):\n",
    "  print(\"Yes, there is at least one match!\")\n",
    "else:\n",
    "  print(\"No match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11', '45']\n",
      "Yes, there is at least one match!\n"
     ]
    }
   ],
   "source": [
    "# [0-5][0-9]\tReturns a match for any two-digit numbers from 00 and 59\n",
    "import re\n",
    "\n",
    "str = \"8 times before 11:45 AM\"\n",
    "\n",
    "#Check if the string has any two-digit numbers, from 00 to 59:\n",
    "\n",
    "x = re.findall(\"[0-5][0-9]\", str)\n",
    "\n",
    "print(x)\n",
    "\n",
    "if (x):\n",
    "  print(\"Yes, there is at least one match!\")\n",
    "else:\n",
    "  print(\"No match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t', 'i', 'm', 'e', 's', 'b', 'e', 'f', 'o', 'r', 'e', 'A', 'M']\n",
      "Yes, there is at least one match!\n"
     ]
    }
   ],
   "source": [
    "#[a-zA-Z]\tReturns a match for any character alphabetically between a and z, lower case OR upper case\n",
    "import re\n",
    "\n",
    "str = \"8 times before 11:45 AM\"\n",
    "\n",
    "#Check if the string has any characters from a to z lower case, and A to Z upper case:\n",
    "\n",
    "x = re.findall(\"[a-zA-Z]\", str)\n",
    "\n",
    "print(x)\n",
    "\n",
    "if (x):\n",
    "  print(\"Yes, there is at least one match!\")\n",
    "else:\n",
    "  print(\"No match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "No match\n"
     ]
    }
   ],
   "source": [
    "# [+]\tIn sets, +, *, ., |, (), $,{} has no special meaning, so [+] means: return a match for any + character in the string\n",
    "import re\n",
    "\n",
    "str = \"8 times before 11:45 AM\"\n",
    "#Check if the string has any + characters:\n",
    "\n",
    "x = re.findall(\"[+]\", str)\n",
    "\n",
    "print(x)\n",
    "\n",
    "if (x):\n",
    "  print(\"Yes, there is at least one match!\")\n",
    "else:\n",
    "  print(\"No match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 NATURAL LANGUAGE PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table of Contents\n",
    "\n",
    "1)Introduction to NLP\n",
    "2)Text Preprocessing\n",
    "    •Noise Removal\n",
    "    •Lexicon Normalization\n",
    "             •Lemmatization\n",
    "             •Stemming\n",
    "    •Object Standardization\n",
    "3)Text to Features (Feature Engineering on text data)\n",
    "    •Syntactical Parsing\n",
    "             •Dependency Grammar\n",
    "             •Part of Speech Tagging\n",
    "    •Entity Parsing\n",
    "             •Phrase Detection\n",
    "             •Named Entity Recognition\n",
    "             •Topic Modelling\n",
    "             •N-Grams\n",
    "    •Statistical features\n",
    "             •TF – IDF\n",
    "             •Frequency / Density Features\n",
    "             •Readability Features\n",
    "    •Word Embeddings\n",
    "4)Important tasks of NLP\n",
    "    •Text Classification\n",
    "    •Text Matching\n",
    "             •Levenshtein Distance\n",
    "             •Phonetic Matching\n",
    "             •Flexible String Matching\n",
    "    •Coreference Resolution\n",
    "    •Other Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1)Introduction to NLP\n",
    "   NLP is a branch of data science that consists of systematic processes for analyzing, understanding, and deriving \n",
    "   information from the text data in a smart and efficient manner. By utilizing NLP and its components, one can organize the \n",
    "   massive chunks of text data, perform numerous automated tasks and solve a wide range of problems such as – automatic \n",
    "   summarization, machine  translation, named entity recognition, relationship extraction, sentiment analysis, speech \n",
    "   recognition, and topic segmentation  etc.\n",
    "     some important terms -\n",
    "        •Tokenization – process of converting a text into tokens\n",
    "        •Tokens – words or entities present in the text\n",
    "        •Text object – a sentence or a phrase or a word or an article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nlp library (NLTK)\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2) Text Preprocessing\n",
    "   Since, text is the most unstructured form of all the available data, various types of noise are present in it and the data \n",
    "   is not readily analyzable without any pre-processing. The entire process of cleaning and standardization of text, making it \n",
    "   noise-free and ready for analysis is known as text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Noise Removal\n",
    " \n",
    "following are the steps for cleaning data- \n",
    "1) First we take the input of raw text data\n",
    "2) removing stopwords , url's , puntuations etc.\n",
    "3) tokenization , lemmatization , stemming\n",
    "4) regular expresion , lookup tables\n",
    "5) cleaned text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing (NLP) is the ability of a computer program to understand human language as it is spoken. \n",
      "     NLP is a component of artificial intelligence (AI).\n"
     ]
    }
   ],
   "source": [
    "#importing our text file from system\n",
    "with open('C:/Users/Dolpy/Desktop/nlp_text.txt', 'r') as input:\n",
    "    data = input.read()\n",
    "print(data) # this file is str type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords what ar they ?\n",
    "  A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, \n",
    "  both when indexing entries for searching and when retrieving them as the result of a search query.\n",
    "       We would not want these words taking up space in our database, or taking up valuable processing time. For this, \n",
    "  we can remove them easily, by storing a list of words that you consider to be stop words.\n",
    "\n",
    "Note: You can even modify the list by adding words of your choice in the english .txt. file in the stopwords directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dolpy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords # importing stopwords\n",
    "#stop words also know as noise in our data\n",
    "\n",
    "#this is how we can downlad stopwords given by NLTK library\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading stop words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "# Showing stop words\n",
    "stop_words[:5]\n",
    "#this is how our stop words looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'the', 'ability', 'of', 'a', 'computer', 'program', 'to', 'understand', 'human', 'language', 'as', 'it', 'is', 'spoken', '.', 'NLP', 'is', 'a', 'component', 'of', 'artificial', 'intelligence', '(', 'AI', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "#for removing stopwords we have to tokenize our txt data. This is how we can do this\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokens = word_tokenize(data)\n",
    "print(word_tokens)dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'ability', 'computer', 'program', 'understand', 'human', 'language', 'spoken', '.', 'NLP', 'component', 'artificial', 'intelligence', '(', 'AI', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "#removing stopwords from our text file \n",
    "#note this data is not cleaned  yet .we can add stopwords to our file to remove other stopwords . or add a list like in our \n",
    "# next example\n",
    "\n",
    "# filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "  \n",
    "filtered_sentence = [] \n",
    "  \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w) \n",
    "  \n",
    "#print(word_tokens) \n",
    "print(filtered_sentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "#Another example of removing stop words\n",
    "#in this  we write our text sentence here without importing it\n",
    "#we can also use regular expression to remove the noise in this .we'll learn regular expression later in this article \n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "  \n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "  \n",
    "stop_words =set(stopwords.words('english')) \n",
    "\n",
    "# we can also write the noise_words / stop words liks this\n",
    "\n",
    "#noise_list = [\"is\", \"a\", \"this\", \"...\"] \n",
    "\n",
    "word_tokens = word_tokenize(example_sent) \n",
    "  \n",
    "# filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "  \n",
    "filtered_sentence = [] \n",
    "  \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w) \n",
    "  \n",
    "print(word_tokens) \n",
    "print(filtered_sentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Lexicon Normalization ###########"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Lexicon Normalization   \n",
    "Another type of textual noise is about the multiple representations exhibited by single word.\n",
    "\n",
    "For example – “play”, “player”, “played”, “plays” and “playing” are the different variations of the word – “play”, \n",
    "Though they mean different but contextually all are similar. The step converts all the disparities of a word into their \n",
    "normalized form (also known as lemma). Normalization is a pivotal step for feature engineering with text as it converts the \n",
    "high dimensional features (N different features) to the low dimensional space (1 feature), which is an ideal ask for any ML \n",
    "model.\n",
    "\n",
    "The most common lexicon normalization practices are :\n",
    "\n",
    "•Stemming:  Stemming is a rudimentary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word.\n",
    "\n",
    "•Lemmatization: Lemmatization, on the other hand, is an organized & step by step procedure of obtaining the root form of the \n",
    "word, it makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar \n",
    "relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Examples of lemmatization:\n",
    "\n",
    "-> rocks : rock\n",
    "-> corpora : corpus\n",
    "-> better : good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cact\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "run\n",
      "run\n",
      "run\n",
      "running\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "#for lemmatizating\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# before using this we have to download content of it.\n",
    "# nltk.download('wordnet') # you can download using this code\n",
    "#here pos means part of speech . noun is the default pos  as you ca see in our examples. But if we want to lemmatize a word \n",
    "#which is noun but  in pos we call it a verb. It will return a same word .\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cact\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"run\",'v'))\n",
    "print(lemmatizer.lemmatize(\"running\",'v'))\n",
    "print(lemmatizer.lemmatize(\"running\",'n'))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "#\n",
    "# we can also pass long sentences like this just by creating a varabble of it and then tokenise ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multipli\n"
     ]
    }
   ],
   "source": [
    "#FOR STEMMING \n",
    "from nltk.stem.porter import PorterStemmer \n",
    "stem = PorterStemmer()\n",
    "\n",
    "word = \"multiplying\" \n",
    "print(stem.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "•Object Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      Text data often contains words or phrases which are not present in any standard lexical dictionaries. These pieces are not \n",
    "recognized by search engines and models.\n",
    "     Some of the examples are – acronyms, hashtags with attached words, and colloquial slangs. With the help of regular \n",
    "expressions and manually prepared data dictionaries, this type of noise can be fixed, the code below uses a dictionary lookup\n",
    "method to replace social media slangs from a text."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "After a text is obtained, we start with text normalization. Text normalization includes:\n",
    "1)converting all letters to lower or upper case\n",
    "2)converting numbers into words(3 to three , 4 to four) or removing numbers\n",
    "3)removing punctuations, accent marks and other diacritics\n",
    "4)removing white spaces\n",
    "5)expanding abbreviations\n",
    "6)removing stop words, sparse terms, and particular words\n",
    "7) ------------------\n",
    "8)removing html links -\n",
    "9)tokenization\n",
    "10)Stemming and or Lemmatization\n",
    "11)Removal of Expressions:\n",
    "12)Split Attached Words:\n",
    "13)Grammar checking:       # this one is complex find api for it\n",
    "14)Spelling correction: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 5 biggest countries by population in 2017 are china, india, united states, indonesia, and brazil.\n"
     ]
    }
   ],
   "source": [
    "#1) convertt to lower case\n",
    "input_str = 'The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.'\n",
    "input_str = input_str.lower()\n",
    "print(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box A contains  red and  white balls, while Box B contains  red and  blue balls.\n"
     ]
    }
   ],
   "source": [
    "#2) removing number from text\n",
    "# this is done using regular expression # we'll learn about regular expression later in tis article\n",
    "import re\n",
    "input_str = 'Box A contains 3 red and 5 white balls, while Box B contains 4 red and 2 blue balls.'\n",
    "result = re.sub(r'\\d+', '', input_str)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey', 'how', 'are', 'you', 'buddy']\n"
     ]
    }
   ],
   "source": [
    "#3)removing punctuations, accent marks and other diacritics\n",
    "\n",
    "#this is also done using regex or regular expression which we'll learn later in this article\n",
    "#There are 14 punctuation marks that are commonly used in English grammar. They are the period, question mark, \n",
    "#       exclamation point, comma, semicolon, colon, dash, hyphen, parentheses, brackets, braces, apostrophe, quotation marks, \n",
    "#        and ellipsis.\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "result = tokenizer.tokenize('hey! how are you ? buddy')\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a string example'"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4)removing white spaces\n",
    "\n",
    "# Whitespace just means characters which are used for spacing, and have an \"empty\" representation.\n",
    "# ex - '\\n' - newline   ,    Newline , ' ' - Space  , etc\n",
    "\n",
    "input_str = ' \\t a string example\\t '\n",
    "string_without_whitespace =input_str.strip()\n",
    "string_without_whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States and Great Britain are ...\n"
     ]
    }
   ],
   "source": [
    "# 5) removing abbreviation\n",
    "\n",
    "#abbreviation refers to a shortened form of a word or phrase. \"SKU is the abbreviation for Stock Keeping Unit\"\n",
    "\n",
    "text='USA and GB are ...'\n",
    "abbrevs={'USA':'United States','GB':'Great Britain'}\n",
    "for abbrev in abbrevs:\n",
    "    text= text.replace(abbrev,abbrevs[abbrev])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6)removing stop \n",
    "# already discuss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The link to this post is '"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8)removing html links\n",
    "import re\n",
    "text ='''The link to this post is https://stackoverflow.com/questions/11331982/how-to-remove-any-url-within-a-string-in-python'''\n",
    "re.sub(r'http://\\S+|https://\\S+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9)tokenization\n",
    "\n",
    "# already discussed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10)Stemming and or Lemmatization\n",
    "#already dicussed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11)Removal of Expressions: \n",
    "\n",
    "#Textual data (usually speech transcripts) may contain human expressions like [laughing], [Crying], [Audience paused]. \n",
    "#These expressions are usually non relevant to content of the speech and hence need to be removed. Simple regular expression \n",
    "\n",
    "\n",
    "#use regualr expression for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12)Split Attached Words: \n",
    "#    We humans in the social forums generate text data, which is completely informal in nature. Most of the tweets are \n",
    "#    accompanied with multiple attached words like RainyDay, PlayingInTheCold etc. These entities can be split into their \n",
    "#    normal forms using simple rules and regex.\n",
    "\n",
    "#use regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#13)Slangs lookup:\n",
    "#   Again, social media comprises of a majority of slang words. These words should be transformed into standard words to \n",
    "#   make free text. The words like luv will be converted to love, Helo to Hello. The similar approach of apostrophe look up can \n",
    "#   be used to convert slangs to standard words. A number of sources are available on the web, which provides lists of all\n",
    "#   possible slangs, this would be your holy grail and you could use them as lookup dictionaries for conversion purposes. \n",
    "tweet =  'I love my <3 iphone & you are awesome apple. Display Is Awesome, sooo happppppy' \n",
    "\n",
    "#use regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to Features (Feature Engineering on text data)\n",
    "To analyse a preprocessed data, it needs to be converted into features. Depending upon the usage, text features can be constructed using assorted techniques – Syntactical Parsing, Entities / N-grams / word-based features, Statistical features, and word embeddings. Read on to understand these techniques in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the NLP domain, a parser is the program or, more specifically, tool that takes natural language in the form of a \n",
    "# sentence or sequence of tokens. It breaks the input stream into smaller chunks. This will help us understand the syntactic\n",
    "# role of each element present in the stream and the basic syntax-level meaning of the sentence. In NLP, a parser actually\n",
    "# analyzes the sentences using the rules of context-free grammar or probabilistic context-free grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Syntactic Parsing\n",
    "Syntactical parsing involves the analysis of words in the sentence for grammar and their arrangement in a manner that \n",
    "shows the relationships among the words. Dependency Grammar and Part of Speech tags are the important attributes of text \n",
    "syntactics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependency Trees\n",
    "Sentences are composed of some words sewed together. The relationship among the words in a sentence is determined by the \n",
    "basic  dependency grammar. Dependency grammar is a class of syntactic text analysis that deals with (labeled) asymmetrical \n",
    "binary relations between two lexical items (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part of speech tagging – \n",
    "Apart from the grammar relations, every word in a sentence is also associated with a part of speech (pos) tag (nouns, verbs,\n",
    "adjectives, adverbs etc). The pos tags defines the usage and function of a word in the sentence. H ere is a list of all \n",
    "possible pos-tags defined by Pennsylvania university. Following code using NLTK performs pos tagging annotation on input\n",
    "text. (it provides several implementations, the default one is perceptron tagger)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('on', 'IN'), ('Analytics', 'NNP'), ('Vidhya', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "#Exmple of POS       #to use this we have download this first # nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#the verb (VB), the noun (NN), the pronoun (PR+DT), the adjective (JJ), the adverb (RB), the preposition (IN), \n",
    "#the conjunction (CC), and the interjection (UH).\n",
    "\n",
    "#we use this for name entity extraction for exmaple - to find out the person name from this we take noun\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "text = \"I am learning Natural Language Processing on Analytics Vidhya\"\n",
    "tokens = word_tokenize(text)\n",
    "print(pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Part of Speech tagging is used for many important purposes in NLP:\n",
    "\n",
    "a).Word sense disambiguation: Some language words have multiple meanings according to their usage. For example, \n",
    "      in the two sentences below:\n",
    "\n",
    "         I. “Please book my flight for Delhi”\n",
    "\n",
    "         II. “I am going to read this book in the flight”\n",
    "\n",
    "“Book” is used with different context, however the part of speech tag for both of the cases are different. In sentence I, \n",
    "the word “book” is used as v erb, while in II it is used as no un. (Lesk Algorithm is also us ed for similar purposes)\n",
    "\n",
    "b)Improving word-based features: A learning model could learn different contexts of a word when used word as the features, \n",
    "    however if the part of speech tag is linked with them, the context is preserved, thus making strong features. For example:\n",
    "\n",
    "c) Normalization and Lemmatization: POS tags are the basis of lemmatization process for converting a word to its base form \n",
    "   (lemma).\n",
    "\n",
    "d) Efficient stopword removal : POS tags are also useful in efficient removal of stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity Extraction (Entities as features)\n",
    "\n",
    "Entities are defined as the most important chunks of a sentence – noun phrases, verb phrases or both. Entity Detection \n",
    "algorithms are generally ensemble models of rule based parsing, dictionary lookups, pos tagging and dependency parsing. \n",
    "The applicability of entity detection can be seen in the automated chat bots, content analyzers and consumer insights.\n",
    "\n",
    "for exmple - for this sentence -  \n",
    "           at the w  party thursday night  at chateau mormount , cate blanchett barely made it up in the elevator\n",
    "    thurrsday is the day\n",
    "    night is time\n",
    "    chateau mormounts is location\n",
    "    cate blanchett is a person \n",
    "\n",
    "#Topic Modelling & Named Entity Recognition are the two key entity detection methods in NLP.\n",
    "##############\n",
    "1)Named Entity Recognition (NER)\n",
    "The process of detecting the named entities such as person names, location names, company names etc from the text is called \n",
    "as NER\n",
    "\n",
    "Sentence – Sergey Brin, the manager of Google Inc. is walking in the streets of New York.\n",
    "\n",
    "Named Entities –  ( “person” : “Sergey Brin” ), (“org” : “Google Inc.”), (“location” : “New York”)\n",
    "\n",
    "###########\n",
    "A typical NER model consists of three blocks:\n",
    "\n",
    "a)Noun phrase identification: This step deals with extracting all the noun phrases from a text using dependency parsing and \n",
    "part of speech tagging .\n",
    "\n",
    "b)Phrase classification: This is the classification step in which all the extracted noun phrases are classified into respective \n",
    "categories (locations, names etc). Google Maps API provides a good path to disambiguate locations, Then, the open\n",
    "databases from dbpedia, wikipedia can be used to identify person names or company names. Apart from this, one can curate \n",
    "the lookup tables and dictionaries by combining information from different sources.\n",
    "\n",
    "c)Entity disambiguation: Sometimes it is possible that entities are misclassified, hence creating a validation layer on top \n",
    "    of  the results is useful. Use of knowledge graphs can be exploited for this purposes. The popular knowledge graphs are \n",
    "– Google Knowledge Graph, IBM Watson and Wikipedia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lets learn entity extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('European', 'JJ'),\n",
       " ('authorities', 'NNS'),\n",
       " ('fined', 'VBD'),\n",
       " ('Google', 'NNP'),\n",
       " ('a', 'DT'),\n",
       " ('record', 'NN'),\n",
       " ('$', '$'),\n",
       " ('5.1', 'CD'),\n",
       " ('billion', 'CD'),\n",
       " ('on', 'IN'),\n",
       " ('Wednesday', 'NNP'),\n",
       " ('for', 'IN'),\n",
       " ('abusing', 'VBG'),\n",
       " ('its', 'PRP$'),\n",
       " ('power', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('mobile', 'JJ'),\n",
       " ('phone', 'NN'),\n",
       " ('market', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('ordered', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('company', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('alter', 'VB'),\n",
       " ('its', 'PRP$'),\n",
       " ('practices', 'NNS')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing useful libraries we need for entity reconition\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "#our data\n",
    "#ex = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and \n",
    "#ordered the company to alter its practices'\n",
    "\n",
    "\n",
    "ex = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and  ordered the company to alter its practices'\n",
    "\n",
    "#apply word tokenization and part-of-speech tagging to the sentence.\n",
    "\n",
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return (sent)\n",
    "sent = preprocess(ex)\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  European/JJ\n",
      "  authorities/NNS\n",
      "  fined/VBD\n",
      "  Google/NNP\n",
      "  (NP a/DT record/NN)\n",
      "  $/$\n",
      "  5.1/CD\n",
      "  billion/CD\n",
      "  on/IN\n",
      "  Wednesday/NNP\n",
      "  for/IN\n",
      "  abusing/VBG\n",
      "  its/PRP$\n",
      "  (NP power/NN)\n",
      "  in/IN\n",
      "  (NP the/DT mobile/JJ phone/NN)\n",
      "  (NP market/NN)\n",
      "  and/CC\n",
      "  ordered/VBD\n",
      "  (NP the/DT company/NN)\n",
      "  to/TO\n",
      "  alter/VB\n",
      "  its/PRP$\n",
      "  practices/NNS)\n"
     ]
    }
   ],
   "source": [
    "#this is the process of chunking what is chunking?\n",
    "\n",
    "#The basic technique we will use for entity detection is chunking, which segments and labels multi-token sequences as illustrated\n",
    "#in 2.1. The smaller boxes show the word-level tokenization and part-of-speech tagging, while the large boxes show higher-level \n",
    "#chunking. Each of these larger boxes is called a chunk. Like tokenization, which omits whitespace, chunking usually selects a \n",
    "#subset of the tokens. Also like tokenization, the pieces produced by a chunker do not overlap in the source text.\n",
    "\n",
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(sent)\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2)Topic Modeling\n",
    "Topic modeling is a process of automatically identifying the topics present in a text corpus, it derives the hidden patterns \n",
    "among the words in the corpus in an unsupervised manner. Topics are defined as “a repeating pattern of co-occurring terms in\n",
    "a corpus”. A good topic model results in – “health”, “doctor”, “patient”, “hospital” for a topic – Healthcare, and “farm”, \n",
    "“crops”, “wheat” for a topic – “Farming”.\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) is the most popular topic modelling technique, Following is the code to implement topic \n",
    "modeling using LDA in python. \n",
    "\n",
    "#lets learn topic modeling \n",
    "#from https://towardsdatascience.com/nlp-extracting-the-main-topics-from-your-dataset-using-lda-in-minutes-21486f5aa925\n",
    "\n",
    "#important topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is', 'a', 'foo')\n",
      "('is', 'a', 'foo', 'bar')\n",
      "('a', 'foo', 'bar', 'sentences')\n",
      "('foo', 'bar', 'sentences', 'and')\n",
      "('bar', 'sentences', 'and', 'i')\n",
      "('sentences', 'and', 'i', 'want')\n",
      "('and', 'i', 'want', 'to')\n",
      "('i', 'want', 'to', 'ngramize')\n",
      "('want', 'to', 'ngramize', 'it')\n"
     ]
    }
   ],
   "source": [
    "#  N-Grams as Features\n",
    "#n-gram is a very popular and widely used technique in the NLP domain. If you are dealing with text data or speech data, \n",
    "#you can use this concept.Let's look at the formal definition of n-grams. An n-gram is a continuous sequence of n items from \n",
    "#the given sequence of text data or speech data  .There are some versions of n-grams that you will find very useful. If we \n",
    "#put n=1, then that particular n-gram is referred to as a unigram. If we put n=2, then we get the bigram. If we  put n=3, then \n",
    "#that particular n-gram is referred to as a trigram, and if you put n=4 or n=5, then these versions of n-grams are referred to\n",
    "#as four gram and five gram, respectively\n",
    "# If you are dealing with text data or speech data, you can use this concept.\n",
    "\n",
    "from nltk import ngrams\n",
    "sentence =  \"this is a foo bar sentences and i want to ngramize it\"\n",
    "n=4\n",
    "ngramres = ngrams(sentence.split() , n)\n",
    "for grams in ngramres:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "The concept TF-IDF stands for term frequency-inverse document frequency. This is in the field of numerical statistics. With \n",
    "this concept, we will be able to decide how important a word is to a given document in the present dataset or corpus.\n",
    "   This is a very simple but useful concept. It actually indicates how many times a particular word appears in the dataset and \n",
    "what the importance of the word is in order to understand the document or dataset. Let's give you an example. Suppose you have \n",
    "a dataset where students write an essay on the topic, My Car. In this dataset, the word a appears many times; it's a high \n",
    "frequency word compared to other words in the dataset. The dataset contains other words like car, shopping, and so on that  \n",
    "appear less often, so their frequencyare lower and they carry more information compared to the word, a. This is the intuition \n",
    "behind TF-IDF.\n",
    "\n",
    "•Term Frequency (TF) – TF for a term “t” is defined as the count of a term “t” in a document “D”\n",
    "•Inverse Document Frequency (IDF) – IDF for a term is defined as logarithm of ratio of total documents available in the corpus\n",
    " and number of documents containing the term T.\n",
    "  \n",
    "    Let's explain this concept in detail. Let's also look at its mathematical aspect. TF-IDF has two parts: Term Frequency \n",
    "and Inverse Document Frequency. Let's begin with the term frequency. The term is self-explanatory but we will walk through the\n",
    "concept. The term frequency indicates the frequency of each of the words present in the document or dataset. So, its equation \n",
    "is given as follows:\n",
    "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "\n",
    "      Now let's look at the second part - inverse document frequency. IDF actually tells us how important the word is to the \n",
    "document. This is because when we calculate TF, we give equal importance to every single word. Now, if the word appears in the\n",
    "dataset more frequently, then its term frequency (TF) value is high while not being that important to the document.So, if the \n",
    "word the appears in the document 100 times, then it's not carrying that much information compared to words that are less \n",
    "frequent in the dataset. Thus, we need to define some weighing down of the frequent terms while scaling up the rare ones, which\n",
    "decides the importance of each word. We will achieve this with the following equation:\n",
    "    \n",
    "    IDF(t) = log10(Total number of documents / Number of documents with term t in it).\n",
    "\n",
    "So, our equation is calculate TF-IDF is as follows.\n",
    "\n",
    "    TF * IDF = [ (Number of times term t appears in a document) / (Total number of terms in the\n",
    "                             document) ] * log10(Total number of documents / Number of documents with term t in it).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t0.5844829010200651\n",
      "  (0, 2)\t0.5844829010200651\n",
      "  (0, 4)\t0.444514311537431\n",
      "  (0, 1)\t0.34520501686496574\n",
      "  (1, 1)\t0.3853716274664007\n",
      "  (1, 0)\t0.652490884512534\n",
      "  (1, 3)\t0.652490884512534\n",
      "  (2, 4)\t0.444514311537431\n",
      "  (2, 1)\t0.34520501686496574\n",
      "  (2, 6)\t0.5844829010200651\n",
      "  (2, 5)\t0.5844829010200651\n"
     ]
    }
   ],
   "source": [
    "#The model creates a vocabulary dictionary and assigns an index to each word. Each row in the output contains a tuple (i,j) \n",
    "#and a tf-idf value of word at index j in document i.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "obj = TfidfVectorizer()\n",
    "corpus = ['This is sample document.', 'another random document.', 'third sample document text']\n",
    "X = obj.fit_transform(corpus)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B. Count / Density / Readability Features\n",
    "Count or Density based features can also be used in models and analysis. These features might seem trivial but shows a great \n",
    "impact in learning models. Some of the features are: Word Count, Sentence Count, Punctuation Counts and Industry specific word \n",
    "counts. Other types of measures include readability measures such as syllable counts, smog index and flesch reading ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Embedding (text vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word embedding is the modern way of representing words as vectors. The aim of word embedding is to redefine the high \n",
    "dimensional word features into low dimensional feature vectors by preserving the contextual similarity in the corpus. They are \n",
    "widely used in deep learning models such as Convolutional Neural Networks and Recurrent Neural Networks.\n",
    "    Word2Vec and GloVe are the two popular models to create word embedding of a text. These models takes a text corpus as input\n",
    "and produces the word vectors as output.\n",
    "\n",
    "    Word2Vec model is composed of preprocessing module, a shallow neural network model called Continuous Bag of Words and \n",
    "another shallow neural network model called skip-gram. These models are widely used for all other nlp problems. It first \n",
    "constructs a vocabulary from the training corpus and then learns word embedding representations. Following code using gensim \n",
    "package prepares the word embedding as the vectors.\n",
    "\n",
    "#another defination\n",
    "# Word Embedding is a language modeling technique used for mapping words to vectors of real numbers. It represents words or phrases \n",
    "# in vector space with several dimensions. Word embeddings can be generated using various methods like neural networks,\n",
    "# co-occurrence matrix, probabilistic models, etc.\n",
    "\n",
    "Word2Vec consists of models for generating word embedding. These models are shallow two layer neural networks having one input layer,\n",
    "one hidden layer and one output layer. Word2Vec utilizes two architectures :\n",
    "1)CBOW (Continuous Bag of Words):\n",
    "2)Skip Gram :\n",
    "for both goto -\n",
    "https://www.geeksforgeeks.org/python-word-embedding-using-word2vec/\n",
    "https://iksinc.online/tag/continuous-bag-of-words-cbow/\n",
    "https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-cbow.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.1 Text Classification\n",
    "\n",
    "Text classification is one of the classical problem of NLP. Notorious examples include – Email Spam Identification, topic \n",
    "classification of news, sentiment classification and organization of web pages by search engines.\n",
    "\n",
    "Text classification, in common words is defined as a technique to systematically classify a text object (document or sentence) in \n",
    "one of the fixed category. It is really helpful when the amount of data is too large, especially for organizing, information \n",
    "filtering, and storage purposes.\n",
    "\n",
    "A typical natural language classifier consists of two parts: (a) Training (b) Prediction as shown in image below. Firstly the text \n",
    "input is processes and features are created. The machine learning models then learn these features and is used for predicting\n",
    "against the new text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text classification \n",
    "#steps involve in text classification\n",
    "# 1)import data\n",
    "# 2) word count- countvectorizer\n",
    "# 3) TF - ITF\n",
    "# 4) naive bayes classsifier\n",
    "# 5)result\n",
    "\n",
    "#first lets unerstand structure of our input data.\n",
    "  we have fetch20newsgroup data in which we have train and test folder . it has variuos catogaries of topics like sports , tech etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing our data_Set directly \n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med']\n",
    "news_train = fetch_20newsgroups(subset='train' , categories= categories , shuffle=True)\n",
    "news_test = fetch_20newsgroups(subset='test' , categories= categories , shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what is dictonaries and its keys ? here's an example -\n",
    "#sample_dict = {key1:'value1'\n",
    "#               key2:'value2'       }\n",
    "# so key is the ket in the dictonary and value for that ket is is value in our sample_dict\n",
    "#if we type sample_dict.keys() it will give us the all key values and sample_dict.values() will give us all the values in sample_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "#this is how we'll get keys in our text \n",
    "print(news_train.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n"
     ]
    }
   ],
   "source": [
    "#this is how we'll get target names\n",
    "print(news_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now our next step is  word count- countvectorizer\n",
    "\n",
    "#in this we'll count a number of times a words is comming in a particular catogary . So how it is done? here's how\n",
    "#suppose we have a very \n",
    "#big text data we'll provide each of unique number to each of unique words and then it will count that how \n",
    "#many times a particular number is repeating\n",
    "\n",
    "#giving each word a unique number is done by fit method  (this is under countvectorizer class) and then \n",
    "#counting a number of times a particular word is coming is done by transform method \n",
    "\n",
    "# lets see example of it here of Word Count - CountVectorize\n",
    "\n",
    "#We have a text - \"The quick brown fox jumped over the lazy dog.\"\n",
    "#Assign a unique number to each word as: also known as \"Tokenize\"\n",
    "\n",
    "#{'the': 7, 'lazy': 4, 'jumped': 3, 'brown': 0, 'over': 5, 'quick': 6, 'dog': 1, 'fox': 2}\n",
    "#Features are: (Vocabulary) [ 8 features]\n",
    "\n",
    "#['brown', 'dog', 'fox', 'jumped', 'lazy', 'over', 'quick', 'the']\n",
    "\n",
    "#In ML terms: Learn a vocabulary dictionary of all tokens in the raw documents,and it is done by using CountVectorizer.fit()\n",
    "\n",
    "#Count the occurence of each word: basically in ML terms \"encoding documents\"\n",
    "\n",
    "#It is done by using CountVectorizer.transform() \n",
    "\n",
    "# it will give this output \n",
    "\n",
    "# [[1 1 1 1 1 1 1 2]]\n",
    "#['brown', 'dog', 'fox', 'jumped', 'lazy', 'over', 'quick', 'the']\n",
    "\n",
    "#It stores it as an array and its shape is: (1,8) i.e 1 no. of sample and 8 no. of features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print Vocabulary :{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "\n",
      "\n",
      "Features names : ['brown', 'dog', 'fox', 'jumped', 'lazy', 'over', 'quick', 'the']\n",
      "\n",
      "\n",
      "The shape of count is :(3, 8)\n",
      "\n",
      "\n",
      "printing count:\n",
      "[[1 1 1 1 1 1 1 2]\n",
      " [0 1 0 0 0 0 0 1]\n",
      " [0 0 1 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "#demo code to understand  CountVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\" ,\n",
    "        \"The dog.\" , \n",
    "        \"The fox\" ]\n",
    "vector = CountVectorizer()\n",
    "vector.fit(text)           # it gives unique number to each and every word\n",
    "\n",
    "#learn a vocabulary dictionary of all tokens in the raw documents .\n",
    "print(\"print Vocabulary :\" + str(vector.vocabulary_) + '\\n\\n')\n",
    "      \n",
    "vector.get_feature_names()\n",
    "print(\"Features names : \" + str(vector.get_feature_names()) +'\\n\\n' )   #getting features \n",
    "      \n",
    "counts = vector.transform(text)        #count the number of occurence\n",
    "print(\"The shape of count is :\" + str(counts.shape)+'\\n\\n')\n",
    "# only 1 sample space and it has 8 features\n",
    "print(\"printing count:\" + '\\n' + str(counts.toarray()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 20 newpaper continuation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_tf = count_vect.fit_transform(news_train.data) #we can also use fit and tranform in one go\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaning frequency of all featurers:[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.        ]\n",
      "\n",
      "\n",
      "Transforming the metrix based on learnt frequency or weight:\n",
      "\n",
      "[[0.36388646 0.27674503 0.27674503 0.36388646 0.36388646 0.36388646\n",
      "  0.36388646 0.42983441]\n",
      " [0.         0.78980693 0.         0.         0.         0.\n",
      "  0.         0.61335554]\n",
      " [0.         0.         0.78980693 0.         0.         0.\n",
      "  0.         0.61335554]]\n"
     ]
    }
   ],
   "source": [
    "#we have counted a number of words it occurs but it has one issue that is word like 'the' it will come so man times .But its count\n",
    "#doesn't make any sense in the classification of the document so we'll use TF and ITF here. what it does is TF(term frequency) just \n",
    "# count the number of times a word is coming and what idf does is it gives weight to words .\n",
    "#lets take a exmples and see what they really are\n",
    "\n",
    "\n",
    "#demo code to undertand TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#create the transform\n",
    "\n",
    "vectorizer = TfidfTransformer()\n",
    "vectorizer.fit(counts)\n",
    "\n",
    "\n",
    "print(\"leaning frequency of all featurers:\" +str(vectorizer.idf_)+'\\n\\n')\n",
    "\n",
    "\n",
    "freq = vectorizer.transform(counts)\n",
    "print(\"Transforming the metrix based on learnt frequency or weight:\\n\\n\" +str(freq.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets continue our classification from here\n",
    "# 20 newpaper continuation\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_tf) #we can also use fit and tranform in one go\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in this step we'll implement naive bayes classifier\n",
    "#SKLEARN already has inbuilt Multinomial Naive Bayes Classiifer package (we'll use this)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf , news_train.target) #fit is used to learn thats why we haven't use transform here \n",
    "#(X_train_tfidf this is our input and news_train.target this is our taget output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1], dtype=int64)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now our model is ready lets do some classification on the basis of text (prediciction of what type of text it belngs to )\n",
    "docs_new = ['God is love' , 'OPENGL on the Glu is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 3, ..., 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tf = count_vect.transform(news_test.data)\n",
    "X_Test_tfidf = tfidf_transformer.transform(X_test_tf)\n",
    "predicted = clf.predict(X_train_tfidf)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import  metrics\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# print(\"Accuracy:\" , accuracy_score(news_test.target , predicted))\n",
    "# print(metrics.classification_report(news_test.target , predicted , target_names=news_test.target_names)),\n",
    "# metrics.confusion_matrix(news_test.target, predicted)\n",
    "\n",
    "#this will give you precision recall value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy achieved is 0.8348868175765646\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.97      0.60      0.74       319\n",
      "         comp.graphics       0.96      0.89      0.92       389\n",
      "               sci.med       0.97      0.81      0.88       396\n",
      "soc.religion.christian       0.65      0.99      0.78       398\n",
      "\n",
      "             micro avg       0.83      0.83      0.83      1502\n",
      "             macro avg       0.89      0.82      0.83      1502\n",
      "          weighted avg       0.88      0.83      0.84      1502\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[192,   2,   6, 119],\n",
       "       [  2, 347,   4,  36],\n",
       "       [  2,  11, 322,  61],\n",
       "       [  2,   2,   1, 393]], dtype=int64)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can write all this code in shorts here's how\n",
    "#we dont have to use count vectorizer and tdidf seperatly we can use it combinly usinf tdidf vectorizer\n",
    "# now we can also use  tdidf vectorizer and a=our naice bayes toger using pipeline like this\n",
    "\n",
    "import sklearn.datasets as skd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "text_clf = Pipeline([('vect', TfidfVectorizer()), \n",
    "                      ('clf', MultinomialNB()) ])\n",
    "\n",
    "# train the model\n",
    "text_clf.fit(news_train.data, news_train.target)\n",
    "# Predict the test cases\n",
    "predicted = text_clf.predict(news_test.data)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "print('Accuracy achieved is ' + str(np.mean(predicted == news_test.target)))\n",
    "print(metrics.classification_report(news_test.target, predicted, target_names=news_test.target_names)),\n",
    "metrics.confusion_matrix(news_test.target, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we already know that what is precision recall f1- score but what is support micro avg macro avg and weighted avg ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#that means that god is love belngs to 3rd catogory which is religion catogary and OPENGL on the Glu is fast belongs to graphics \n",
    "#         catogary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=1.0, subjectivity=1.0)\n",
      "Sentiment(polarity=0.7, subjectivity=0.6000000000000001)\n"
     ]
    }
   ],
   "source": [
    "#sentimental analysis\n",
    "\n",
    "#Sentiment analysis is basically the process of determining the attitude or the emotion of the writer, i.e., whether it is\n",
    "#positive or negative or neutral.\n",
    "#The sentiment function of textblob returns two properties, polarity, and subjectivity.\n",
    "\n",
    "#Polarity is float which lies in the range of [-1,1] where 1 means positive statement and -1 means a negative statement. Subjective\n",
    "#sentences generally refer to personal opinion, emotion or judgment whereas objective refers to factual information. Subjectivity is\n",
    "#also a float which lies in the range of [0,1].\n",
    "\n",
    "\n",
    "#there are four steps involve before we dd our sentimental analysis these are-\n",
    "#tokeniztion \n",
    "#cleaning the data\n",
    "#removing stop words\n",
    "#classification\n",
    "# so we have do these steps to do before sentimental analysis\n",
    "from textblob import TextBlob\n",
    "feed1 = \" the food at radison was awesome\"\n",
    "feed2 =\"the food at redison was good\"\n",
    "blob1 = TextBlob(feed1) \n",
    "blob2 = TextBlob(feed2)\n",
    "print(blob1.sentiment)\n",
    "print(blob2.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytics vidhya\n",
      "great platform\n",
      "data science\n"
     ]
    }
   ],
   "source": [
    "#Noun Phrase Extraction\n",
    "# we can just extract out the noun phrases from the textblob. Noun Phrase extraction is particularly important when you want to \n",
    "#analyze the “who” in a sentence. Lets see an example below.\n",
    "\n",
    "#As we can see that the results aren’t perfectly correct, but we should be aware that we are working with machines.\n",
    "\n",
    "blob = TextBlob(\"Analytics Vidhya is a great platform to learn data science.\")\n",
    "for np in blob.noun_phrases:\n",
    "    print(np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Analytics Vidhya is a great platform to learn data science\")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spelling Correction\n",
    "#Spelling correction is a cool feature which TextBlob offers, we can be accessed using the correct function as shown below.\n",
    "\n",
    "blob = TextBlob('Analytics Vidhya is a gret platfrm to learn data scence')\n",
    "blob.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great', 0.5351351351351351),\n",
       " ('get', 0.3162162162162162),\n",
       " ('grew', 0.11216216216216217),\n",
       " ('grey', 0.026351351351351353),\n",
       " ('greet', 0.006081081081081081),\n",
       " ('fret', 0.002702702702702703),\n",
       " ('grit', 0.0006756756756756757),\n",
       " ('cret', 0.0006756756756756757)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can also check the list of suggested word and its confidence using the spellcheck function.\n",
    "blob.words[4].spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Translation and Language Detection\n",
    "\n",
    "#used to detect language\n",
    "blob.detect_language() \n",
    "\n",
    "#use to translate one language to another \n",
    "blob.translate(from_lang='ar', to ='en') #here ar means arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=0kPRaYSgblM #for text classification\n",
    "# https://www.youtube.com/watch?v=zi16nl82AMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The spam filter in your email and the spellcheck you’ve used since you learned to type in elementary school are some other basic examples of when your computer is understanding language.As a data scientist, we may use NLP for sentiment analysis (classifying words to have positive or negative connotation) or to make predictions in classification models, among other things.\n"
     ]
    }
   ],
   "source": [
    "# text summarization using summarization\n",
    "\n",
    "text = \"NLP is short for Natural Language Processing. As you probably know, computers are not as great at understanding words as they are numbers. This is all changing though as advances in NLP are happening everyday. The fact that devices like Apple’s Siri and Amazon’s Alexa can (usually) comprehend when we ask the weather, for directions, or to play a certain genre of music are all examples of NLP. The spam filter in your email and the spellcheck you’ve used since you learned to type in elementary school are some other basic examples of when your computer is understanding language.As a data scientist, we may use NLP for sentiment analysis (classifying words to have positive or negative connotation) or to make predictions in classification models, among other things. Typically, whether we’re given the data or have to scrape it, the text will be in its natural human format of sentences, paragraphs, tweets, etc. From there, before we can dig into analyzing, we will have to do some cleaning to break the text down into a format the computer can easily understand.\"\n",
    "\n",
    "from gensim.summarization import summarize\n",
    "from gensim.summarization import keywords\n",
    "\n",
    "print(summarize(text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for more learning goto-\n",
    "https://www.nltk.org/book/\n",
    "https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
